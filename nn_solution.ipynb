{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YfeFAfgJg2bG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import lzma\n",
        "from itertools import islice\n",
        "import regex as re\n",
        "import sys\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch import nn\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thFxvj7IhUHu",
        "outputId": "c6b69dcf-65a0-4895-a387-20962c25ae32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bIm0QpdHhVCg"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/ModelowanieJezyka/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_BhbM7o6nYo5"
      },
      "outputs": [],
      "source": [
        "with open(path + \"challenging-america-word-gap-prediction/train/expected.tsv\") as file:\n",
        "  expected = file.readlines()\n",
        "\n",
        "expected = [x[:-1] for x in expected]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JGvVwJKlhXOb"
      },
      "outputs": [],
      "source": [
        "def get_words_from_line(line, count):\n",
        "  line = line.rstrip()\n",
        "  line = line.split(\"\\t\")\n",
        "  text = line[-2] + \" \" + expected[count] + \" \" + line[-1]\n",
        "  text = re.sub(r\"\\\\+n\", \" \", text)\n",
        "  text = re.sub('[^A-Za-z ]+', '', text)\n",
        "  for t in text.split():\n",
        "    yield t\n",
        "\n",
        "\n",
        "def get_word_lines_from_file(file_name):\n",
        "  count = 0\n",
        "  with lzma.open(file_name, encoding='utf8', mode=\"rt\") as fh:\n",
        "    for line in fh:\n",
        "       yield get_words_from_line(line, count)\n",
        "       count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PnsHdzAisSch"
      },
      "outputs": [],
      "source": [
        "vocab_size = 25_000\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    get_word_lines_from_file(path + \"challenging-america-word-gap-prediction/train/in.tsv.xz\"),\n",
        "    max_tokens = vocab_size,\n",
        "    specials = ['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kmcD0qKNf0B0"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "WORDS_LEN = 61\n",
        "HALF_WORDS_LEN = WORDS_LEN // 2\n",
        "\n",
        "def look_ahead_iterator(gen):\n",
        "   words = deque()\n",
        "   for item in gen:\n",
        "      if len(words) == WORDS_LEN:\n",
        "         first_part = tuple(itertools.islice(words, 0, HALF_WORDS_LEN))\n",
        "         second_part = tuple(itertools.islice(words, HALF_WORDS_LEN+1, None))\n",
        "         concat = first_part + second_part\n",
        "         yield (concat, words[HALF_WORDS_LEN])\n",
        "         words.popleft()\n",
        "         words.append(item)\n",
        "      else:\n",
        "        words.append(item)\n",
        "\n",
        "class FullLines(IterableDataset):\n",
        "  def __init__(self, text_file, vocabulary_size):\n",
        "      self.vocab = vocab\n",
        "      self.vocab.set_default_index(self.vocab['<unk>'])\n",
        "      self.vocabulary_size = vocabulary_size\n",
        "      self.text_file = text_file\n",
        "\n",
        "  def __iter__(self):\n",
        "     return look_ahead_iterator(\n",
        "         (self.vocab[t] for t in itertools.chain.from_iterable(get_word_lines_from_file(self.text_file))))\n",
        "\n",
        "train_dataset = FullLines(path + \"challenging-america-word-gap-prediction/train/in.tsv.xz\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XK7Q3XF9hegE"
      },
      "outputs": [],
      "source": [
        "embed_size = 300\n",
        "\n",
        "class FinalNNModel(nn.Module):\n",
        "  def __init__(self, vocabulary_size, embedding_size, hl_size ,second_hl_size):\n",
        "      super(FinalNNModel, self).__init__()\n",
        "      self.general_embeddings = nn.Embedding(vocabulary_size, embedding_size)\n",
        "      self.embeddings = nn.Embedding(vocabulary_size, embedding_size)\n",
        "      self.dropout = nn.Dropout(p=0.3)\n",
        "      self.hl = nn.Linear(embedding_size*7, hl_size)\n",
        "      self.second_hl = nn.Linear(hl_size, second_hl_size)\n",
        "      self.output_layer = nn.Linear(second_hl_size, vocabulary_size)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      emb_0 = self.embeddings(x[HALF_WORDS_LEN-3])\n",
        "      emb_1 = self.embeddings(x[HALF_WORDS_LEN-2])\n",
        "      emb_2 = self.embeddings(x[HALF_WORDS_LEN-1])\n",
        "      emb_3 = self.embeddings(x[HALF_WORDS_LEN])\n",
        "      emb_4 = self.embeddings(x[HALF_WORDS_LEN+1])\n",
        "      emb_5 = self.embeddings(x[HALF_WORDS_LEN+2])\n",
        "\n",
        "      first_ge = torch.mean(self.general_embeddings(torch.stack(x[:HALF_WORDS_LEN-3], dim=0)), dim=0)\n",
        "      second_ge = torch.mean(self.general_embeddings(torch.stack(x[HALF_WORDS_LEN+3:], dim=0)), dim=0)\n",
        "      \n",
        "      general_embeddings = torch.mean(torch.stack([first_ge, second_ge]), dim=0)\n",
        "\n",
        "      number_of_dimensions = len(emb_1.size())\n",
        "      x = torch.cat([emb_0, emb_1, emb_2, emb_3, emb_4, emb_5, general_embeddings], dim=number_of_dimensions-1)\n",
        "      \n",
        "      x = self.hl(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.second_hl(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.output_layer(x)\n",
        "      x = nn.LogSoftmax(dim=number_of_dimensions-1)(x)\n",
        "      return x\n",
        "\n",
        "hl_size = 1000\n",
        "second_hl_size = 500\n",
        "\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_4fNaPDhfu1",
        "outputId": "e0a9d7c9-9261-49d1-d1cd-76510d2aa68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tensor(10.1469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "100 tensor(670.1851, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "200 tensor(600.6952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "300 tensor(574.3068, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "400 tensor(555.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "500 tensor(542.8818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "600 tensor(531.1751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "700 tensor(521.6666, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "800 tensor(515.3813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "900 tensor(508.7432, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1000 tensor(503.4024, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1100 tensor(498.4164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1200 tensor(493.4991, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1300 tensor(488.9643, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1400 tensor(486.8802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1500 tensor(481.1358, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1600 tensor(481.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1700 tensor(478.9554, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1800 tensor(475.5597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1900 tensor(473.4020, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2000 tensor(467.3519, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2100 tensor(468.4716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2200 tensor(464.2966, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2300 tensor(464.0510, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2400 tensor(462.7190, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2500 tensor(462.3445, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2600 tensor(459.4728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2700 tensor(457.8138, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2800 tensor(457.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "2900 tensor(455.7231, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3000 tensor(455.5038, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3100 tensor(453.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3200 tensor(452.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3300 tensor(451.1621, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3400 tensor(449.2247, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3500 tensor(449.7834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3600 tensor(445.4313, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3700 tensor(447.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3800 tensor(447.7823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "3900 tensor(445.5927, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4000 tensor(444.5666, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4100 tensor(443.3255, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4200 tensor(443.1867, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4300 tensor(441.4759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4400 tensor(440.5734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4500 tensor(440.9001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4600 tensor(440.3218, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4700 tensor(437.7694, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4800 tensor(436.7120, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "4900 tensor(435.7605, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5000 tensor(437.7459, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5100 tensor(438.4003, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5200 tensor(435.9197, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5300 tensor(434.1667, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5400 tensor(434.1755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5500 tensor(433.5568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5600 tensor(433.9248, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5700 tensor(432.9419, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5800 tensor(431.4171, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "5900 tensor(431.9984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6000 tensor(432.8610, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6100 tensor(431.1475, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6200 tensor(431.6252, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6300 tensor(430.3545, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6400 tensor(429.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6500 tensor(430.6871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6600 tensor(429.0574, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6700 tensor(427.5727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6800 tensor(430.2888, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "6900 tensor(426.4834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7000 tensor(427.9198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7100 tensor(425.3973, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7200 tensor(425.9273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7300 tensor(427.9070, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7400 tensor(425.8397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7500 tensor(423.4171, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7600 tensor(428.9877, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7700 tensor(423.7862, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7800 tensor(424.7999, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "7900 tensor(425.4449, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8000 tensor(425.1149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8100 tensor(423.6315, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8200 tensor(422.9811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8300 tensor(421.3521, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8400 tensor(421.8823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8500 tensor(422.3034, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8600 tensor(422.3286, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8700 tensor(422.7229, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8800 tensor(423.2242, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "8900 tensor(423.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9000 tensor(421.8037, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9100 tensor(419.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9200 tensor(420.2855, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9300 tensor(421.7102, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9400 tensor(419.4128, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9500 tensor(421.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9600 tensor(419.4063, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9700 tensor(420.1884, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9800 tensor(418.8954, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "9900 tensor(418.6916, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10000 tensor(418.1534, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10100 tensor(418.7393, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10200 tensor(419.1331, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10300 tensor(418.4997, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10400 tensor(420.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10500 tensor(416.7169, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10600 tensor(417.4247, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10700 tensor(417.9044, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10800 tensor(415.5677, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "10900 tensor(418.8569, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11000 tensor(414.3222, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11100 tensor(415.9987, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11200 tensor(417.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11300 tensor(417.6726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11400 tensor(416.4893, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11500 tensor(414.8634, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11600 tensor(415.4134, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11700 tensor(414.4344, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11800 tensor(415.5331, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "11900 tensor(416.3517, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12000 tensor(413.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12100 tensor(414.2355, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12200 tensor(415.7674, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12300 tensor(413.0664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12400 tensor(413.3604, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12500 tensor(412.9593, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12600 tensor(412.4401, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12700 tensor(413.6330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12800 tensor(412.5138, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "12900 tensor(413.6582, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13000 tensor(413.6961, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13100 tensor(412.9454, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13200 tensor(412.3408, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13300 tensor(412.1203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13400 tensor(412.4858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13500 tensor(411.3181, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "13600 tensor(413.2943, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model = FinalNNModel(vocab_size, embed_size, hl_size, second_hl_size).to(device)\n",
        "data = DataLoader(train_dataset, batch_size=10_000)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = torch.nn.NLLLoss()\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "stacked_loss = 0\n",
        "state = None\n",
        "\n",
        "for x, y in data:\n",
        "   x = [x[idx].to(device) for idx in range(len(x))]\n",
        "   y = y.to(device)\n",
        "\n",
        "   optimizer.zero_grad()\n",
        "   y_predicted = model(x)\n",
        "   loss = criterion(y_predicted, y)\n",
        "   stacked_loss += loss\n",
        "\n",
        "   if step % 100 == 0:\n",
        "      print(step, stacked_loss)\n",
        "      stacked_loss = 0\n",
        "\n",
        "   step += 1\n",
        "   loss.backward()\n",
        "   optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pLw4zJVwhiiG"
      },
      "outputs": [],
      "source": [
        "vocab_unique = set(vocab.get_stoi().keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xIMJCF0-hjXk"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "output = []\n",
        "x = 0\n",
        "with lzma.open(path + \"challenging-america-word-gap-prediction/test-A/in.tsv.xz\", encoding='utf8', mode=\"rt\") as file:\n",
        "    for line in file:\n",
        "        line = line.split(\"\\t\")\n",
        "\n",
        "        first_part = re.sub(r\"\\\\+n\", \" \", line[-2])\n",
        "        first_part = re.sub('[^A-Za-z ]+', '', first_part).split()\n",
        "\n",
        "        second_part = re.sub(r\"\\\\+n\", \" \", line[-1])\n",
        "        second_part = re.sub('[^A-Za-z ]+', '', second_part).split()\n",
        "\n",
        "        input_words = first_part[-HALF_WORDS_LEN:] + second_part[:HALF_WORDS_LEN]\n",
        "        input_words = vocab.forward(input_words)\n",
        "        input_tokens = [torch.tensor(q).to(device) for q in input_words]\n",
        "\n",
        "        if len(input_words) < HALF_WORDS_LEN*2:\n",
        "          output.append(\":1.0\\n\")\n",
        "          continue\n",
        "        out = torch.exp(model(input_tokens))\n",
        "\n",
        "        top = torch.topk(out, 100)\n",
        "        top_indices = top.indices.tolist()\n",
        "        top_probs = top.values.tolist()\n",
        "        unk_bonus = 1 - sum(top_probs)\n",
        "        top_words = vocab.lookup_tokens(top_indices)\n",
        "        top_zipped = list(zip(top_words, top_probs))\n",
        "\n",
        "        res = \"\"\n",
        "        for w, p in top_zipped:\n",
        "            if w == \"<unk>\":\n",
        "                res += f\":{(p + unk_bonus + 0.01):.4f} \"\n",
        "            else:\n",
        "                if p > 0.0001:\n",
        "                  res += f\"{w}:{p:.4f} \"\n",
        "        \n",
        "        res = res[:-1]\n",
        "        res += \"\\n\"\n",
        "        output.append(res)\n",
        "\n",
        "with open(path + \"challenging-america-word-gap-prediction/test-A/out.tsv\", mode=\"w\") as file:\n",
        "    file.writelines(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TItFmbrxhlWG"
      },
      "outputs": [],
      "source": [
        "output = []\n",
        "x = 0\n",
        "with lzma.open(path + \"challenging-america-word-gap-prediction/dev-0/in.tsv.xz\", encoding='utf8', mode=\"rt\") as file:\n",
        "    for line in file:\n",
        "        line = line.split(\"\\t\")\n",
        "\n",
        "        first_part = re.sub(r\"\\\\+n\", \" \", line[-2])\n",
        "        first_part = re.sub('[^A-Za-z ]+', '', first_part).split()\n",
        "\n",
        "        second_part = re.sub(r\"\\\\+n\", \" \", line[-1])\n",
        "        second_part = re.sub('[^A-Za-z ]+', '', second_part).split()\n",
        "\n",
        "        input_words = first_part[-HALF_WORDS_LEN:] + second_part[:HALF_WORDS_LEN]\n",
        "        input_words = vocab.forward(input_words)\n",
        "        input_tokens = [torch.tensor(q).to(device) for q in input_words]\n",
        "\n",
        "        if len(input_words) < HALF_WORDS_LEN*2:\n",
        "          output.append(\":1.0\\n\")\n",
        "          continue\n",
        "        out = torch.exp(model(input_tokens))\n",
        "\n",
        "        top = torch.topk(out, 100)\n",
        "        top_indices = top.indices.tolist()\n",
        "        top_probs = top.values.tolist()\n",
        "        unk_bonus = 1 - sum(top_probs)\n",
        "        top_words = vocab.lookup_tokens(top_indices)\n",
        "        top_zipped = list(zip(top_words, top_probs))\n",
        "\n",
        "        res = \"\"\n",
        "        for w, p in top_zipped:\n",
        "            if w == \"<unk>\":\n",
        "                res += f\":{(p + unk_bonus + 0.01):.4f} \"\n",
        "            else:\n",
        "                if p > 0.0001:\n",
        "                  res += f\"{w}:{p:.4f} \"\n",
        "        \n",
        "        res = res[:-1]\n",
        "        res += \"\\n\"\n",
        "        output.append(res)\n",
        "\n",
        "with open(path + \"challenging-america-word-gap-prediction/dev-0/out.tsv\", mode=\"w\") as file:\n",
        "    file.writelines(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
